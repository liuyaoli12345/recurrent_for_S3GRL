{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMLQohxICUd+apSvp2Vuhyx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uwONhu3TRdQP","executionInfo":{"status":"ok","timestamp":1695199044259,"user_tz":-480,"elapsed":4101,"user":{"displayName":"刘尧力","userId":"10993956014691111069"}},"outputId":"67141c82-34e6-4351-b5b9-a6dc6e5af56c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive\n","!ls\n","!mkdir recurrent_for_S3GRL\n","!cd recurrent_for_S3GRL"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MUoFtdxmSKlI","executionInfo":{"status":"ok","timestamp":1695199044259,"user_tz":-480,"elapsed":10,"user":{"displayName":"刘尧力","userId":"10993956014691111069"}},"outputId":"a9d67462-3d34-478a-d275-c875f605fed5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n","'can you tell me something about Taylor Swift'\\''s new album?.gdoc'\n","'Colab Notebooks'\n"," DriveSyncFiles\n"," ML2022\n"," recurrent_for_S3GRL\n"," S3GRL\n","'Untitled Diagram (1).drawio'\n","'Untitled Diagram.drawio'\n","'Untitled spreadsheet.gsheet'\n"," 关于新传前景的讨论.mp4\n"," 咨询建议书.gslides\n"," 咨询建议书.pptx\n","'大创项目 答辩稿01.docx'\n","'大创项目 答辩稿.docx'\n","'大创项目 答辩稿.gdoc'\n"," 婚礼.gslides\n"," 字词检测卡.gslides\n"," 无标题绘图.gdraw\n"," 月度预算.gsheet\n"," 科学项目.gslides\n","mkdir: cannot create directory ‘recurrent_for_S3GRL’: File exists\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/venomouscyanide/S3GRL_OGB.git\n","%cd S3GRL_OGB\n","!bash quick_install.sh -y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZKRPegb3St0C","executionInfo":{"status":"ok","timestamp":1695199522836,"user_tz":-480,"elapsed":478581,"user":{"displayName":"刘尧力","userId":"10993956014691111069"}},"outputId":"51e76adf-0335-42ca-cc8d-2981bc39a756"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'S3GRL_OGB'...\n","remote: Enumerating objects: 4122, done.\u001b[K\n","remote: Counting objects: 100% (868/868), done.\u001b[K\n","remote: Compressing objects: 100% (187/187), done.\u001b[K\n","remote: Total 4122 (delta 697), reused 749 (delta 676), pack-reused 3254\u001b[K\n","Receiving objects: 100% (4122/4122), 36.70 MiB | 11.90 MiB/s, done.\n","Resolving deltas: 100% (2392/2392), done.\n","Updating files: 100% (625/625), done.\n","/content/drive/MyDrive/S3GRL_OGB\n","quick_install.sh: line 3: conda: command not found\n","quick_install.sh: line 4: conda: command not found\n","quick_install.sh: line 5: conda: command not found\n","Collecting torch-sparse==0.6.13\n","  Downloading torch_sparse-0.6.13.tar.gz (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse==0.6.13) (1.11.2)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse==0.6.13) (1.23.5)\n","Building wheels for collected packages: torch-sparse\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for torch-sparse (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for torch-sparse\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for torch-sparse\n","Failed to build torch-sparse\n","\u001b[31mERROR: Could not build wheels for torch-sparse, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0mLooking in links: https://data.pyg.org/whl/torch-1.11.0+cpu.html\n","Collecting torch-scatter\n","  Downloading torch_scatter-2.1.1.tar.gz (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-sparse\n","  Downloading torch_sparse-0.6.17.tar.gz (209 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-spline-conv\n","  Downloading torch_spline_conv-1.2.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.7.22)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n","Building wheels for collected packages: torch-scatter, torch-sparse, torch-spline-conv, torch-geometric\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for torch-scatter (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for torch-scatter\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for torch-scatter\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for torch-sparse (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for torch-sparse\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for torch-sparse\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for torch-spline-conv\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for torch-spline-conv\n","  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=3fe7313b2e0db760ef48342796b931793e57827443cea0375be0882fdffa30db\n","  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n","Successfully built torch-geometric\n","Failed to build torch-scatter torch-sparse torch-spline-conv\n","\u001b[31mERROR: Could not build wheels for torch-scatter, torch-sparse, torch-spline-conv, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0mCollecting matplotlib==3.6.1\n","  Downloading matplotlib-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.1) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.1) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.1) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.1) (1.4.5)\n","Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.1) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.1) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.1) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.1) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.6.1) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.6.1) (1.16.0)\n","Installing collected packages: matplotlib\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.7.1\n","    Uninstalling matplotlib-3.7.1:\n","      Successfully uninstalled matplotlib-3.7.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","seaborn 0.12.2 requires matplotlib!=3.6.1,>=3.1, but you have matplotlib 3.6.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed matplotlib-3.6.1\n","Collecting ogb==1.3.5\n","  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb==1.3.5) (2.0.1+cu118)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb==1.3.5) (1.23.5)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb==1.3.5) (4.66.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb==1.3.5) (1.2.2)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb==1.3.5) (1.5.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb==1.3.5) (1.16.0)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb==1.3.5) (2.0.4)\n","Collecting outdated>=0.2.0 (from ogb==1.3.5)\n","  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb==1.3.5) (67.7.2)\n","Collecting littleutils (from outdated>=0.2.0->ogb==1.3.5)\n","  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb==1.3.5) (2.31.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb==1.3.5) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb==1.3.5) (2023.3.post1)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb==1.3.5) (1.11.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb==1.3.5) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb==1.3.5) (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb==1.3.5) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb==1.3.5) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb==1.3.5) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb==1.3.5) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb==1.3.5) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb==1.3.5) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb==1.3.5) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb==1.3.5) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb==1.3.5) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb==1.3.5) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb==1.3.5) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb==1.3.5) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb==1.3.5) (1.3.0)\n","Building wheels for collected packages: littleutils\n","  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7028 sha256=48f6acc9210fba3114c24ab3be5c4c9f759c213aee3b622e3396ece72175d575\n","  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n","Successfully built littleutils\n","Installing collected packages: littleutils, outdated, ogb\n","Successfully installed littleutils-0.2.2 ogb-1.3.5 outdated-0.2.2\n","Collecting networkx==2.8.7\n","  Downloading networkx-2.8.7-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: networkx\n","  Attempting uninstall: networkx\n","    Found existing installation: networkx 3.1\n","    Uninstalling networkx-3.1:\n","      Successfully uninstalled networkx-3.1\n","Successfully installed networkx-2.8.7\n","Collecting pytorch_memlab==0.2.4\n","  Downloading pytorch_memlab-0.2.4.tar.gz (93 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pytorch_memlab==0.2.4) (67.7.2)\n","Collecting calmsize (from pytorch_memlab==0.2.4)\n","  Downloading calmsize-0.1.3.tar.gz (3.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.10/dist-packages (from pytorch_memlab==0.2.4) (1.5.3)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_memlab==0.2.4) (2.0.1+cu118)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->pytorch_memlab==0.2.4) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->pytorch_memlab==0.2.4) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->pytorch_memlab==0.2.4) (1.23.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch_memlab==0.2.4) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch_memlab==0.2.4) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch_memlab==0.2.4) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch_memlab==0.2.4) (2.8.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch_memlab==0.2.4) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->pytorch_memlab==0.2.4) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->pytorch_memlab==0.2.4) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->pytorch_memlab==0.2.4) (16.0.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.18->pytorch_memlab==0.2.4) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->pytorch_memlab==0.2.4) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->pytorch_memlab==0.2.4) (1.3.0)\n","Building wheels for collected packages: pytorch_memlab, calmsize\n","  Building wheel for pytorch_memlab (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytorch_memlab: filename=pytorch_memlab-0.2.4-py3-none-any.whl size=22837 sha256=79a49e3f116a5355741c8b474e17f8e519b3ecff3e77daf55d96f1cb166cc83f\n","  Stored in directory: /root/.cache/pip/wheels/7b/b1/de/ebd0614016892fe3dea206a417114af4f97f6d98ffc860e0fd\n","  Building wheel for calmsize (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for calmsize: filename=calmsize-0.1.3-py3-none-any.whl size=2868 sha256=b1707b2079a037f070f4705ac6699c91792754a5659919e8b08bb767dafe126e\n","  Stored in directory: /root/.cache/pip/wheels/5c/b5/ad/4ec2d714dc114d33dadc848a8874b2c0b33fcf323addc75cd4\n","Successfully built pytorch_memlab calmsize\n","Installing collected packages: calmsize, pytorch_memlab\n","Successfully installed calmsize-0.1.3 pytorch_memlab-0.2.4\n","Collecting class_resolver==0.3.10\n","  Downloading class_resolver-0.3.10-py3-none-any.whl (21 kB)\n","Installing collected packages: class_resolver\n","Successfully installed class_resolver-0.3.10\n","Collecting fast-pagerank==0.0.4\n","  Downloading fast_pagerank-0.0.4-py3-none-any.whl (5.2 kB)\n","Installing collected packages: fast-pagerank\n","Successfully installed fast-pagerank-0.0.4\n","Collecting scikit-learn==1.1.3\n","  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.11.2)\n","Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (3.2.0)\n","Installing collected packages: scikit-learn\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","Successfully installed scikit-learn-1.1.3\n","Collecting graphistry==0.28.5\n","  Downloading graphistry-0.28.5-py3-none-any.whl (193 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.7/193.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphistry==0.28.5) (1.23.5)\n","Collecting palettable>=3.0 (from graphistry==0.28.5)\n","  Downloading palettable-3.3.3-py2.py3-none-any.whl (332 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from graphistry==0.28.5) (1.5.3)\n","Requirement already satisfied: pyarrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from graphistry==0.28.5) (9.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from graphistry==0.28.5) (2.31.0)\n","Collecting squarify (from graphistry==0.28.5)\n","  Downloading squarify-0.4.3-py3-none-any.whl (4.3 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from graphistry==0.28.5) (4.5.0)\n","Requirement already satisfied: packaging>=20.1 in /usr/local/lib/python3.10/dist-packages (from graphistry==0.28.5) (23.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.17.0->graphistry==0.28.5) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.17.0->graphistry==0.28.5) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->graphistry==0.28.5) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->graphistry==0.28.5) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->graphistry==0.28.5) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->graphistry==0.28.5) (2023.7.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.17.0->graphistry==0.28.5) (1.16.0)\n","Installing collected packages: squarify, palettable, graphistry\n","Successfully installed graphistry-0.28.5 palettable-3.3.3 squarify-0.4.3\n","Collecting scipy==1.9.3\n","  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<1.26.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from scipy==1.9.3) (1.23.5)\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.11.2\n","    Uninstalling scipy-1.11.2:\n","      Successfully uninstalled scipy-1.11.2\n","Successfully installed scipy-1.9.3\n","Found existing installation: grpcio 1.57.0\n","Uninstalling grpcio-1.57.0:\n","  Would remove:\n","    /usr/local/lib/python3.10/dist-packages/grpc/*\n","    /usr/local/lib/python3.10/dist-packages/grpcio-1.57.0.dist-info/*\n","Proceed (Y/n)? y\n","  Successfully uninstalled grpcio-1.57.0\n","quick_install.sh: line 18: conda: command not found\n","Collecting ray==2.1.0\n","  Downloading ray-2.1.0-cp310-cp310-manylinux2014_x86_64.whl (58.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from ray==2.1.0) (23.1.0)\n","Collecting click<=8.0.4,>=7.0 (from ray==2.1.0)\n","  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray==2.1.0) (3.12.2)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray==2.1.0) (4.19.0)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray==2.1.0) (1.0.5)\n","Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray==2.1.0) (3.20.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray==2.1.0) (6.0.1)\n","Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray==2.1.0) (1.3.1)\n","Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray==2.1.0) (1.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray==2.1.0) (2.31.0)\n","Collecting virtualenv>=20.0.24 (from ray==2.1.0)\n","  Downloading virtualenv-20.24.5-py3-none-any.whl (3.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting grpcio>=1.42.0 (from ray==2.1.0)\n","  Downloading grpcio-1.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray==2.1.0) (23.1)\n","Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from ray==2.1.0) (1.23.5)\n","Collecting distlib<1,>=0.3.7 (from virtualenv>=20.0.24->ray==2.1.0)\n","  Downloading distlib-0.3.7-py2.py3-none-any.whl (468 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs<4,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.0.24->ray==2.1.0) (3.10.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.1.0) (2023.7.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.1.0) (0.30.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.1.0) (0.10.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.1.0) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.1.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.1.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.1.0) (2023.7.22)\n","Installing collected packages: distlib, virtualenv, grpcio, click, ray\n","  Attempting uninstall: click\n","    Found existing installation: click 8.1.7\n","    Uninstalling click-8.1.7:\n","      Successfully uninstalled click-8.1.7\n","Successfully installed click-8.0.4 distlib-0.3.7 grpcio-1.58.0 ray-2.1.0 virtualenv-20.24.5\n","Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (0.9.0)\n","Collecting tensorboardX==2.5.1\n","  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX==2.5.1) (1.23.5)\n","Collecting protobuf<=3.20.1,>=3.8.0 (from tensorboardX==2.5.1)\n","  Downloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: protobuf, tensorboardX\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-bigquery 3.10.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-bigquery-storage 2.22.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-functions 1.13.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","googleapis-common-protos 1.60.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","grpc-google-iam-v1 0.12.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n","tensorflow 2.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n","tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed protobuf-3.20.1 tensorboardX-2.5.1\n"]}]},{"cell_type":"code","source":["import torch\n","\n","!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n","!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n","!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n","!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n","!pip install git+https://github.com/pyg-team/pytorch_geometric.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wTkrjFUfpCOq","executionInfo":{"status":"ok","timestamp":1695199573879,"user_tz":-480,"elapsed":51047,"user":{"displayName":"刘尧力","userId":"10993956014691111069"}},"outputId":"3aa7e861-87f4-4d8c-ab33-10ed70c9ba2d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.1.1+pt20cu118\n","Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n","Collecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.9.3)\n","Requirement already satisfied: numpy<1.26.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.23.5)\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.17+pt20cu118\n","Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.9.3)\n","Requirement already satisfied: numpy<1.26.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.23.5)\n","Installing collected packages: torch-cluster\n","Successfully installed torch-cluster-1.6.1+pt20cu118\n","Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n","  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-25qv42yx\n","  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-25qv42yx\n","  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 5d81cab11c4a29b8975abd46440f3e85c64dc0d4\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.9.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (3.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (3.1.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.1.3)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric==2.4.0) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (2023.7.22)\n","Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.4.0) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.4.0) (3.2.0)\n","Building wheels for collected packages: torch_geometric\n","  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch_geometric: filename=torch_geometric-2.4.0-py3-none-any.whl size=1012154 sha256=3f21310a4fbc3dd35540eb6cbe392b2f77700a9dd673a7c3f475b53de6baa23a\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-sklfcz9r/wheels/d3/78/eb/9e26525b948d19533f1688fb6c209cec8a0ba793d39b49ae8f\n","Successfully built torch_geometric\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.4.0\n"]}]},{"cell_type":"code","source":["!pip install gtrick\n","!pip install rdkit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvB-QlWwzbAQ","executionInfo":{"status":"ok","timestamp":1695200240701,"user_tz":-480,"elapsed":11726,"user":{"displayName":"刘尧力","userId":"10993956014691111069"}},"outputId":"c29c5eee-144d-4089-fa51-e2578ffbfb2a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gtrick in /usr/local/lib/python3.10/dist-packages (0.0.dev2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from gtrick) (2.0.1+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gtrick) (1.23.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from gtrick) (2.8.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->gtrick) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->gtrick) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->gtrick) (1.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->gtrick) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->gtrick) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->gtrick) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->gtrick) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->gtrick) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->gtrick) (1.3.0)\n","Collecting rdkit\n","  Downloading rdkit-2023.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n","Installing collected packages: rdkit\n","Successfully installed rdkit-2023.3.3\n"]}]},{"cell_type":"code","source":["!python sgrl_run_manager.py --config configs/ogbl/ogbl_collab.json --results_json ogbl_collab_results.json"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y9xF3N73lpEI","outputId":"15be4962-68bd-4498-fc4d-e2670e8e6d9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.\n","Run 1 of ogbl-collab with id ogbl_collab_pos_plus_11_uvai using device cuda:0\n","Current arguments accepted are: {'M': 0,\n"," 'base_gae': '',\n"," 'batch_size': 64,\n"," 'cache_dynamic': False,\n"," 'calc_ratio': False,\n"," 'checkpoint_training': False,\n"," 'continue_from': None,\n"," 'cuda_device': 0,\n"," 'data_appendix': '',\n"," 'dataset': 'ogbl-collab',\n"," 'dataset_split_num': 1,\n"," 'dataset_stats': False,\n"," 'delete_dataset': False,\n"," 'device': device(type='cuda', index=0),\n"," 'dropedge': 0.0,\n"," 'dropout': 0.5,\n"," 'dynamic_test': True,\n"," 'dynamic_train': True,\n"," 'dynamic_val': True,\n"," 'edge_feature': '',\n"," 'epochs': 10,\n"," 'eval_steps': 1,\n"," 'fast_split': False,\n"," 'hidden_channels': 1024,\n"," 'init_features': 'n2v',\n"," 'init_representation': '',\n"," 'k_heuristic': 1,\n"," 'k_node_set_strategy': 'intersection',\n"," 'k_pool_strategy': 'mean',\n"," 'keep_old': True,\n"," 'log_steps': 1,\n"," 'loss_fn': '',\n"," 'lr': 0.0001,\n"," 'm': 0,\n"," 'max_nodes_per_hop': None,\n"," 'model': 'SIGN',\n"," 'n2v_dim': 16,\n"," 'neg_ratio': 1,\n"," 'node_label': 'zo',\n"," 'normalize_feats': False,\n"," 'num_hops': 1,\n"," 'num_layers': -1,\n"," 'num_workers': 70,\n"," 'only_test': False,\n"," 'optimize_sign': True,\n"," 'pairwise': False,\n"," 'pool_operatorwise': True,\n"," 'pretrained_node_embedding': None,\n"," 'profile': False,\n"," 'ratio_per_hop': 1,\n"," 'runs': 1,\n"," 'save_appendix': '',\n"," 'seed': 1,\n"," 'sign_k': 1,\n"," 'sign_type': 'PoS',\n"," 'size_only': False,\n"," 'sortpool_k': -1,\n"," 'split_by_year': True,\n"," 'split_test_ratio': 0.1,\n"," 'split_val_ratio': 0.05,\n"," 'test_multiple_models': False,\n"," 'test_percent': 100,\n"," 'train_gae': False,\n"," 'train_mf': False,\n"," 'train_mlp': False,\n"," 'train_n2v': False,\n"," 'train_node_embedding': False,\n"," 'train_percent': 100,\n"," 'use_edge_weight': False,\n"," 'use_feature': True,\n"," 'use_heuristic': None,\n"," 'use_mlp': False,\n"," 'use_valedges_as_input': True,\n"," 'val_percent': 100}\n","Results will be saved in results/ogbl-collab_20230920085953_seed1\n","Command line input: python sgrl_run_manager.py --config configs/ogbl/ogbl_collab.json --results_json ogbl_collab_results.json\n"," is saved.\n","Filtering ogbl-collab training set to >= 2010 year\n","Adding validation edges to training edges\n","Init features using: n2v\n","Prepping n2v embeddings with hidden_dim: 16\n","  0%|                                          | 0/10 [00:00<?, ?it/s]Epoch: 01, Step: 100/7371, Loss: 2.7330\n","Epoch: 01, Step: 200/7371, Loss: 2.4378\n","Epoch: 01, Step: 300/7371, Loss: 2.3882\n","Epoch: 01, Step: 400/7371, Loss: 1.9753\n","Epoch: 01, Step: 500/7371, Loss: 1.9520\n","Epoch: 01, Step: 600/7371, Loss: 1.8296\n","Epoch: 01, Step: 700/7371, Loss: 1.8839\n","Epoch: 01, Step: 800/7371, Loss: 1.6014\n","Epoch: 01, Step: 900/7371, Loss: 1.5780\n","Epoch: 01, Step: 1000/7371, Loss: 1.3877\n","Epoch: 01, Step: 1100/7371, Loss: 1.4885\n","Epoch: 01, Step: 1200/7371, Loss: 1.4092\n","Epoch: 01, Step: 1300/7371, Loss: 1.2430\n","Epoch: 01, Step: 1400/7371, Loss: 1.2561\n","Epoch: 01, Step: 1500/7371, Loss: 1.2661\n","Epoch: 01, Step: 1600/7371, Loss: 1.1734\n","Epoch: 01, Step: 1700/7371, Loss: 1.1369\n","Epoch: 01, Step: 1800/7371, Loss: 1.1370\n","Epoch: 01, Step: 1900/7371, Loss: 1.1013\n","Epoch: 01, Step: 2000/7371, Loss: 1.0704\n","Epoch: 01, Step: 2100/7371, Loss: 1.0545\n","Epoch: 01, Step: 2200/7371, Loss: 1.0110\n","Epoch: 01, Step: 2300/7371, Loss: 1.0509\n","Epoch: 01, Step: 2400/7371, Loss: 1.0476\n","Epoch: 01, Step: 2500/7371, Loss: 1.0736\n","Epoch: 01, Step: 2600/7371, Loss: 1.0571\n","Epoch: 01, Step: 2700/7371, Loss: 1.0648\n","Epoch: 01, Step: 2800/7371, Loss: 1.0196\n","Epoch: 01, Step: 2900/7371, Loss: 1.0328\n","Epoch: 01, Step: 3000/7371, Loss: 1.0323\n","Epoch: 01, Step: 3100/7371, Loss: 0.9625\n","Epoch: 01, Step: 3200/7371, Loss: 0.9486\n","Epoch: 01, Step: 3300/7371, Loss: 0.9743\n","Epoch: 01, Step: 3400/7371, Loss: 0.9827\n","Epoch: 01, Step: 3500/7371, Loss: 0.9908\n","Epoch: 01, Step: 3600/7371, Loss: 1.0195\n","Epoch: 01, Step: 3700/7371, Loss: 1.0204\n","Epoch: 01, Step: 3800/7371, Loss: 0.9859\n","Epoch: 01, Step: 3900/7371, Loss: 1.0237\n","Epoch: 01, Step: 4000/7371, Loss: 1.0036\n","Epoch: 01, Step: 4100/7371, Loss: 0.9933\n","Epoch: 01, Step: 4200/7371, Loss: 0.9667\n","Epoch: 01, Step: 4300/7371, Loss: 0.9927\n","Epoch: 01, Step: 4400/7371, Loss: 0.9874\n","Epoch: 01, Step: 4500/7371, Loss: 0.9780\n","Epoch: 01, Step: 4600/7371, Loss: 0.9951\n","Epoch: 01, Step: 4700/7371, Loss: 1.0131\n","Epoch: 01, Step: 4800/7371, Loss: 0.9804\n","Epoch: 01, Step: 4900/7371, Loss: 1.0102\n","Epoch: 01, Step: 5000/7371, Loss: 1.0334\n","Epoch: 01, Step: 5100/7371, Loss: 1.0162\n","Epoch: 01, Step: 5200/7371, Loss: 0.9794\n","Epoch: 01, Step: 5300/7371, Loss: 1.0048\n","Epoch: 01, Step: 5400/7371, Loss: 0.9970\n","Epoch: 01, Step: 5500/7371, Loss: 0.9585\n","Epoch: 01, Step: 5600/7371, Loss: 0.9663\n","Epoch: 01, Step: 5700/7371, Loss: 1.0254\n","Epoch: 01, Step: 5800/7371, Loss: 1.0286\n","Epoch: 01, Step: 5900/7371, Loss: 1.0554\n","Epoch: 01, Step: 6000/7371, Loss: 1.0344\n","Epoch: 01, Step: 6100/7371, Loss: 1.0363\n","Epoch: 01, Step: 6200/7371, Loss: 1.0614\n","Epoch: 01, Step: 6300/7371, Loss: 0.9835\n","Epoch: 01, Step: 6400/7371, Loss: 0.9953\n","Epoch: 01, Step: 6500/7371, Loss: 1.0198\n","Epoch: 01, Step: 6600/7371, Loss: 1.0171\n","Epoch: 01, Step: 6700/7371, Loss: 1.0413\n","Epoch: 01, Step: 6800/7371, Loss: 1.0384\n","Epoch: 01, Step: 6900/7371, Loss: 0.9935\n","Epoch: 01, Step: 7000/7371, Loss: 0.9846\n","Epoch: 01, Step: 7100/7371, Loss: 0.9455\n","Epoch: 01, Step: 7200/7371, Loss: 1.0130\n","Epoch: 01, Step: 7300/7371, Loss: 1.0852\n"," 10%|███▍                              | 1/10 [00:32<04:52, 32.54s/it]Epoch: 02, Step: 100/7371, Loss: 0.8696\n","Epoch: 02, Step: 200/7371, Loss: 0.8729\n","Epoch: 02, Step: 300/7371, Loss: 0.8840\n","Epoch: 02, Step: 400/7371, Loss: 0.8650\n","Epoch: 02, Step: 500/7371, Loss: 0.8766\n","Epoch: 02, Step: 600/7371, Loss: 0.8728\n","Epoch: 02, Step: 700/7371, Loss: 0.9054\n","Epoch: 02, Step: 800/7371, Loss: 0.8684\n","Epoch: 02, Step: 900/7371, Loss: 0.8708\n","Epoch: 02, Step: 1000/7371, Loss: 0.9197\n","Epoch: 02, Step: 1100/7371, Loss: 0.9091\n","Epoch: 02, Step: 1200/7371, Loss: 0.8944\n","Epoch: 02, Step: 1300/7371, Loss: 0.8931\n","Epoch: 02, Step: 1400/7371, Loss: 0.8689\n","Epoch: 02, Step: 1500/7371, Loss: 0.8754\n","Epoch: 02, Step: 1600/7371, Loss: 0.8770\n","Epoch: 02, Step: 1700/7371, Loss: 0.8839\n","Epoch: 02, Step: 1800/7371, Loss: 0.8752\n","Epoch: 02, Step: 1900/7371, Loss: 0.8956\n","Epoch: 02, Step: 2000/7371, Loss: 0.8833\n","Epoch: 02, Step: 2100/7371, Loss: 0.9219\n","Epoch: 02, Step: 2200/7371, Loss: 0.8733\n","Epoch: 02, Step: 2300/7371, Loss: 0.8928\n","Epoch: 02, Step: 2400/7371, Loss: 0.8687\n","Epoch: 02, Step: 2500/7371, Loss: 0.8795\n","Epoch: 02, Step: 2600/7371, Loss: 0.8817\n","Epoch: 02, Step: 2700/7371, Loss: 0.8935\n","Epoch: 02, Step: 2800/7371, Loss: 0.8873\n","Epoch: 02, Step: 2900/7371, Loss: 0.9062\n","Epoch: 02, Step: 3000/7371, Loss: 0.8890\n","Epoch: 02, Step: 3100/7371, Loss: 0.9170\n","Epoch: 02, Step: 3200/7371, Loss: 0.8527\n","Epoch: 02, Step: 3300/7371, Loss: 0.9212\n","Epoch: 02, Step: 3400/7371, Loss: 0.9162\n","Epoch: 02, Step: 3500/7371, Loss: 0.9510\n","Epoch: 02, Step: 3600/7371, Loss: 0.9150\n","Epoch: 02, Step: 3700/7371, Loss: 0.8809\n","Epoch: 02, Step: 3800/7371, Loss: 0.8977\n","Epoch: 02, Step: 3900/7371, Loss: 0.9217\n","Epoch: 02, Step: 4000/7371, Loss: 0.8643\n","Epoch: 02, Step: 4100/7371, Loss: 0.9149\n","Epoch: 02, Step: 4200/7371, Loss: 0.8964\n","Epoch: 02, Step: 4300/7371, Loss: 0.9051\n","Epoch: 02, Step: 4400/7371, Loss: 0.9635\n","Epoch: 02, Step: 4500/7371, Loss: 0.9007\n","Epoch: 02, Step: 4600/7371, Loss: 0.9072\n","Epoch: 02, Step: 4700/7371, Loss: 0.9195\n","Epoch: 02, Step: 4800/7371, Loss: 0.9419\n","Epoch: 02, Step: 4900/7371, Loss: 0.9457\n","Epoch: 02, Step: 5000/7371, Loss: 0.8635\n","Epoch: 02, Step: 5100/7371, Loss: 0.9280\n","Epoch: 02, Step: 5200/7371, Loss: 0.9025\n","Epoch: 02, Step: 5300/7371, Loss: 0.9123\n","Epoch: 02, Step: 5400/7371, Loss: 0.9196\n","Epoch: 02, Step: 5500/7371, Loss: 0.9115\n","Epoch: 02, Step: 5600/7371, Loss: 0.8575\n","Epoch: 02, Step: 5700/7371, Loss: 0.9123\n","Epoch: 02, Step: 5800/7371, Loss: 0.9521\n","Epoch: 02, Step: 5900/7371, Loss: 0.9147\n","Epoch: 02, Step: 6000/7371, Loss: 0.9127\n","Epoch: 02, Step: 6100/7371, Loss: 0.9263\n","Epoch: 02, Step: 6200/7371, Loss: 0.9859\n","Epoch: 02, Step: 6300/7371, Loss: 0.9037\n","Epoch: 02, Step: 6400/7371, Loss: 0.9566\n","Epoch: 02, Step: 6500/7371, Loss: 0.9256\n","Epoch: 02, Step: 6600/7371, Loss: 0.9080\n","Epoch: 02, Step: 6700/7371, Loss: 0.9323\n","Epoch: 02, Step: 6800/7371, Loss: 0.9099\n","Epoch: 02, Step: 6900/7371, Loss: 0.9158\n","Epoch: 02, Step: 7000/7371, Loss: 0.8973\n","Epoch: 02, Step: 7100/7371, Loss: 0.9203\n","Epoch: 02, Step: 7200/7371, Loss: 0.9655\n","Epoch: 02, Step: 7300/7371, Loss: 0.9305\n"," 20%|██████▊                           | 2/10 [01:00<03:58, 29.86s/it]Epoch: 03, Step: 100/7371, Loss: 0.8415\n","Epoch: 03, Step: 200/7371, Loss: 0.8401\n","Epoch: 03, Step: 300/7371, Loss: 0.8407\n","Epoch: 03, Step: 400/7371, Loss: 0.8359\n","Epoch: 03, Step: 500/7371, Loss: 0.8233\n","Epoch: 03, Step: 600/7371, Loss: 0.8385\n","Epoch: 03, Step: 700/7371, Loss: 0.8422\n","Epoch: 03, Step: 800/7371, Loss: 0.8366\n","Epoch: 03, Step: 900/7371, Loss: 0.8215\n","Epoch: 03, Step: 1000/7371, Loss: 0.8368\n","Epoch: 03, Step: 1100/7371, Loss: 0.8399\n","Epoch: 03, Step: 1200/7371, Loss: 0.8349\n","Epoch: 03, Step: 1300/7371, Loss: 0.8453\n","Epoch: 03, Step: 1400/7371, Loss: 0.8531\n","Epoch: 03, Step: 1500/7371, Loss: 0.8409\n","Epoch: 03, Step: 1600/7371, Loss: 0.8573\n","Epoch: 03, Step: 1700/7371, Loss: 0.8434\n","Epoch: 03, Step: 1800/7371, Loss: 0.8469\n","Epoch: 03, Step: 1900/7371, Loss: 0.8356\n","Epoch: 03, Step: 2000/7371, Loss: 0.8381\n","Epoch: 03, Step: 2100/7371, Loss: 0.8524\n","Epoch: 03, Step: 2200/7371, Loss: 0.8330\n","Epoch: 03, Step: 2300/7371, Loss: 0.8338\n","Epoch: 03, Step: 2400/7371, Loss: 0.8472\n","Epoch: 03, Step: 2500/7371, Loss: 0.8515\n","Epoch: 03, Step: 2600/7371, Loss: 0.8575\n","Epoch: 03, Step: 2700/7371, Loss: 0.8435\n","Epoch: 03, Step: 2800/7371, Loss: 0.8447\n","Epoch: 03, Step: 2900/7371, Loss: 0.8391\n","Epoch: 03, Step: 3000/7371, Loss: 0.8439\n","Epoch: 03, Step: 3100/7371, Loss: 0.8519\n","Epoch: 03, Step: 3200/7371, Loss: 0.8361\n","Epoch: 03, Step: 3300/7371, Loss: 0.8545\n","Epoch: 03, Step: 3400/7371, Loss: 0.8504\n","Epoch: 03, Step: 3500/7371, Loss: 0.8478\n","Epoch: 03, Step: 3600/7371, Loss: 0.8546\n","Epoch: 03, Step: 3700/7371, Loss: 0.8576\n","Epoch: 03, Step: 3800/7371, Loss: 0.8494\n","Epoch: 03, Step: 3900/7371, Loss: 0.8540\n","Epoch: 03, Step: 4000/7371, Loss: 0.8568\n","Epoch: 03, Step: 4100/7371, Loss: 0.8510\n","Epoch: 03, Step: 4200/7371, Loss: 0.8529\n","Epoch: 03, Step: 4300/7371, Loss: 0.8558\n","Epoch: 03, Step: 4400/7371, Loss: 0.8534\n","Epoch: 03, Step: 4500/7371, Loss: 0.8529\n","Epoch: 03, Step: 4600/7371, Loss: 0.8460\n","Epoch: 03, Step: 4700/7371, Loss: 0.8572\n","Epoch: 03, Step: 4800/7371, Loss: 0.8428\n","Epoch: 03, Step: 4900/7371, Loss: 0.8573\n","Epoch: 03, Step: 5000/7371, Loss: 0.8603\n","Epoch: 03, Step: 5100/7371, Loss: 0.8413\n","Epoch: 03, Step: 5200/7371, Loss: 0.8561\n","Epoch: 03, Step: 5300/7371, Loss: 0.8577\n","Epoch: 03, Step: 5400/7371, Loss: 0.8493\n","Epoch: 03, Step: 5500/7371, Loss: 0.8543\n","Epoch: 03, Step: 5600/7371, Loss: 0.8571\n","Epoch: 03, Step: 5700/7371, Loss: 0.8609\n","Epoch: 03, Step: 5800/7371, Loss: 0.8664\n","Epoch: 03, Step: 5900/7371, Loss: 0.8671\n","Epoch: 03, Step: 6000/7371, Loss: 0.8518\n","Epoch: 03, Step: 6100/7371, Loss: 0.8583\n","Epoch: 03, Step: 6200/7371, Loss: 0.8672\n","Epoch: 03, Step: 6300/7371, Loss: 0.8824\n","Epoch: 03, Step: 6400/7371, Loss: 0.8601\n","Epoch: 03, Step: 6500/7371, Loss: 0.8636\n","Epoch: 03, Step: 6600/7371, Loss: 0.8719\n","Epoch: 03, Step: 6700/7371, Loss: 0.8866\n","Epoch: 03, Step: 6800/7371, Loss: 0.8678\n","Epoch: 03, Step: 6900/7371, Loss: 0.8813\n","Epoch: 03, Step: 7000/7371, Loss: 0.8723\n","Epoch: 03, Step: 7100/7371, Loss: 0.8617\n","Epoch: 03, Step: 7200/7371, Loss: 0.8770\n","Epoch: 03, Step: 7300/7371, Loss: 0.8651\n"," 30%|██████████▏                       | 3/10 [01:29<03:27, 29.59s/it]Epoch: 04, Step: 100/7371, Loss: 0.8324\n","Epoch: 04, Step: 200/7371, Loss: 0.8280\n","Epoch: 04, Step: 300/7371, Loss: 0.8252\n","Epoch: 04, Step: 400/7371, Loss: 0.8434\n","Epoch: 04, Step: 500/7371, Loss: 0.8366\n","Epoch: 04, Step: 600/7371, Loss: 0.8275\n","Epoch: 04, Step: 700/7371, Loss: 0.8120\n","Epoch: 04, Step: 800/7371, Loss: 0.8222\n","Epoch: 04, Step: 900/7371, Loss: 0.8227\n","Epoch: 04, Step: 1000/7371, Loss: 0.8188\n","Epoch: 04, Step: 1100/7371, Loss: 0.8375\n","Epoch: 04, Step: 1200/7371, Loss: 0.8344\n","Epoch: 04, Step: 1300/7371, Loss: 0.8274\n","Epoch: 04, Step: 1400/7371, Loss: 0.8336\n","Epoch: 04, Step: 1500/7371, Loss: 0.8336\n","Epoch: 04, Step: 1600/7371, Loss: 0.8316\n","Epoch: 04, Step: 1700/7371, Loss: 0.8300\n","Epoch: 04, Step: 1800/7371, Loss: 0.8277\n","Epoch: 04, Step: 1900/7371, Loss: 0.8337\n","Epoch: 04, Step: 2000/7371, Loss: 0.8277\n","Epoch: 04, Step: 2100/7371, Loss: 0.8190\n","Epoch: 04, Step: 2200/7371, Loss: 0.8348\n","Epoch: 04, Step: 2300/7371, Loss: 0.8324\n","Epoch: 04, Step: 2400/7371, Loss: 0.8277\n","Epoch: 04, Step: 2500/7371, Loss: 0.8279\n","Epoch: 04, Step: 2600/7371, Loss: 0.8338\n","Epoch: 04, Step: 2700/7371, Loss: 0.8380\n","Epoch: 04, Step: 2800/7371, Loss: 0.8367\n","Epoch: 04, Step: 2900/7371, Loss: 0.8222\n","Epoch: 04, Step: 3000/7371, Loss: 0.8430\n","Epoch: 04, Step: 3100/7371, Loss: 0.8387\n","Epoch: 04, Step: 3200/7371, Loss: 0.8297\n","Epoch: 04, Step: 3300/7371, Loss: 0.8243\n","Epoch: 04, Step: 3400/7371, Loss: 0.8297\n","Epoch: 04, Step: 3500/7371, Loss: 0.8297\n","Epoch: 04, Step: 3600/7371, Loss: 0.8346\n","Epoch: 04, Step: 3700/7371, Loss: 0.8290\n","Epoch: 04, Step: 3800/7371, Loss: 0.8440\n","Epoch: 04, Step: 3900/7371, Loss: 0.8341\n","Epoch: 04, Step: 4000/7371, Loss: 0.8349\n","Epoch: 04, Step: 4100/7371, Loss: 0.8380\n","Epoch: 04, Step: 4200/7371, Loss: 0.8353\n","Epoch: 04, Step: 4300/7371, Loss: 0.8395\n","Epoch: 04, Step: 4400/7371, Loss: 0.8421\n","Epoch: 04, Step: 4500/7371, Loss: 0.8437\n","Epoch: 04, Step: 4600/7371, Loss: 0.8393\n","Epoch: 04, Step: 4700/7371, Loss: 0.8298\n","Epoch: 04, Step: 4800/7371, Loss: 0.8415\n","Epoch: 04, Step: 4900/7371, Loss: 0.8338\n","Epoch: 04, Step: 5000/7371, Loss: 0.8339\n","Epoch: 04, Step: 5100/7371, Loss: 0.8364\n","Epoch: 04, Step: 5200/7371, Loss: 0.8486\n","Epoch: 04, Step: 5300/7371, Loss: 0.8415\n","Epoch: 04, Step: 5400/7371, Loss: 0.8324\n","Epoch: 04, Step: 5500/7371, Loss: 0.8406\n","Epoch: 04, Step: 5600/7371, Loss: 0.8468\n","Epoch: 04, Step: 5700/7371, Loss: 0.8385\n","Epoch: 04, Step: 5800/7371, Loss: 0.8491\n","Epoch: 04, Step: 5900/7371, Loss: 0.8445\n","Epoch: 04, Step: 6000/7371, Loss: 0.8426\n","Epoch: 04, Step: 6100/7371, Loss: 0.8451\n","Epoch: 04, Step: 6200/7371, Loss: 0.8408\n","Epoch: 04, Step: 6300/7371, Loss: 0.8379\n","Epoch: 04, Step: 6400/7371, Loss: 0.8429\n","Epoch: 04, Step: 6500/7371, Loss: 0.8412\n","Epoch: 04, Step: 6600/7371, Loss: 0.8451\n","Epoch: 04, Step: 6700/7371, Loss: 0.8455\n","Epoch: 04, Step: 6800/7371, Loss: 0.8442\n","Epoch: 04, Step: 6900/7371, Loss: 0.8486\n","Epoch: 04, Step: 7000/7371, Loss: 0.8422\n","Epoch: 04, Step: 7100/7371, Loss: 0.8512\n","Epoch: 04, Step: 7200/7371, Loss: 0.8464\n","Epoch: 04, Step: 7300/7371, Loss: 0.8454\n"," 40%|█████████████▌                    | 4/10 [01:58<02:54, 29.16s/it]Epoch: 05, Step: 100/7371, Loss: 0.8254\n","Epoch: 05, Step: 200/7371, Loss: 0.8208\n","Epoch: 05, Step: 300/7371, Loss: 0.8277\n","Epoch: 05, Step: 400/7371, Loss: 0.8325\n","Epoch: 05, Step: 500/7371, Loss: 0.8330\n","Epoch: 05, Step: 600/7371, Loss: 0.8327\n","Epoch: 05, Step: 700/7371, Loss: 0.8266\n","Epoch: 05, Step: 800/7371, Loss: 0.8238\n","Epoch: 05, Step: 900/7371, Loss: 0.8225\n","Epoch: 05, Step: 1000/7371, Loss: 0.8234\n","Epoch: 05, Step: 1100/7371, Loss: 0.8302\n","Epoch: 05, Step: 1200/7371, Loss: 0.8195\n","Epoch: 05, Step: 1300/7371, Loss: 0.8295\n","Epoch: 05, Step: 1400/7371, Loss: 0.8247\n","Epoch: 05, Step: 1500/7371, Loss: 0.8303\n","Epoch: 05, Step: 1600/7371, Loss: 0.8253\n","Epoch: 05, Step: 1700/7371, Loss: 0.8281\n","Epoch: 05, Step: 1800/7371, Loss: 0.8294\n","Epoch: 05, Step: 1900/7371, Loss: 0.8260\n","Epoch: 05, Step: 2000/7371, Loss: 0.8328\n","Epoch: 05, Step: 2100/7371, Loss: 0.8339\n","Epoch: 05, Step: 2200/7371, Loss: 0.8340\n","Epoch: 05, Step: 2300/7371, Loss: 0.8278\n","Epoch: 05, Step: 2400/7371, Loss: 0.8181\n","Epoch: 05, Step: 2500/7371, Loss: 0.8322\n","Epoch: 05, Step: 2600/7371, Loss: 0.8282\n","Epoch: 05, Step: 2700/7371, Loss: 0.8283\n","Epoch: 05, Step: 2800/7371, Loss: 0.8369\n","Epoch: 05, Step: 2900/7371, Loss: 0.8285\n","Epoch: 05, Step: 3000/7371, Loss: 0.8311\n","Epoch: 05, Step: 3100/7371, Loss: 0.8363\n","Epoch: 05, Step: 3200/7371, Loss: 0.8336\n","Epoch: 05, Step: 3300/7371, Loss: 0.8372\n","Epoch: 05, Step: 3400/7371, Loss: 0.8328\n","Epoch: 05, Step: 3500/7371, Loss: 0.8257\n","Epoch: 05, Step: 3600/7371, Loss: 0.8291\n","Epoch: 05, Step: 3700/7371, Loss: 0.8330\n","Epoch: 05, Step: 3800/7371, Loss: 0.8328\n","Epoch: 05, Step: 3900/7371, Loss: 0.8391\n","Epoch: 05, Step: 4000/7371, Loss: 0.8438\n","Epoch: 05, Step: 4100/7371, Loss: 0.8375\n","Epoch: 05, Step: 4200/7371, Loss: 0.8305\n","Epoch: 05, Step: 4300/7371, Loss: 0.8371\n","Epoch: 05, Step: 4400/7371, Loss: 0.8434\n","Epoch: 05, Step: 4500/7371, Loss: 0.8338\n","Epoch: 05, Step: 4600/7371, Loss: 0.8376\n","Epoch: 05, Step: 4700/7371, Loss: 0.8361\n","Epoch: 05, Step: 4800/7371, Loss: 0.8367\n","Epoch: 05, Step: 4900/7371, Loss: 0.8356\n","Epoch: 05, Step: 5000/7371, Loss: 0.8403\n","Epoch: 05, Step: 5100/7371, Loss: 0.8325\n","Epoch: 05, Step: 5200/7371, Loss: 0.8389\n","Epoch: 05, Step: 5300/7371, Loss: 0.8326\n","Epoch: 05, Step: 5400/7371, Loss: 0.8272\n","Epoch: 05, Step: 5500/7371, Loss: 0.8416\n","Epoch: 05, Step: 5600/7371, Loss: 0.8366\n","Epoch: 05, Step: 5700/7371, Loss: 0.8383\n","Epoch: 05, Step: 5800/7371, Loss: 0.8412\n","Epoch: 05, Step: 5900/7371, Loss: 0.8418\n","Epoch: 05, Step: 6000/7371, Loss: 0.8393\n","Epoch: 05, Step: 6100/7371, Loss: 0.8426\n","Epoch: 05, Step: 6200/7371, Loss: 0.8446\n","Epoch: 05, Step: 6300/7371, Loss: 0.8414\n","Epoch: 05, Step: 6400/7371, Loss: 0.8406\n","Epoch: 05, Step: 6500/7371, Loss: 0.8449\n","Epoch: 05, Step: 6600/7371, Loss: 0.8446\n","Epoch: 05, Step: 6700/7371, Loss: 0.8330\n","Epoch: 05, Step: 6800/7371, Loss: 0.8447\n","Epoch: 05, Step: 6900/7371, Loss: 0.8537\n","Epoch: 05, Step: 7000/7371, Loss: 0.8541\n","Epoch: 05, Step: 7100/7371, Loss: 0.8434\n","Epoch: 05, Step: 7200/7371, Loss: 0.8435\n","Epoch: 05, Step: 7300/7371, Loss: 0.8500\n"," 50%|█████████████████                 | 5/10 [02:27<02:25, 29.10s/it]Epoch: 06, Step: 100/7371, Loss: 0.8182\n","Epoch: 06, Step: 200/7371, Loss: 0.8281\n","Epoch: 06, Step: 300/7371, Loss: 0.8271\n","Epoch: 06, Step: 400/7371, Loss: 0.8132\n","Epoch: 06, Step: 500/7371, Loss: 0.8283\n","Epoch: 06, Step: 600/7371, Loss: 0.8207\n","Epoch: 06, Step: 700/7371, Loss: 0.8235\n","Epoch: 06, Step: 800/7371, Loss: 0.8156\n","Epoch: 06, Step: 900/7371, Loss: 0.8281\n","Epoch: 06, Step: 1000/7371, Loss: 0.8258\n","Epoch: 06, Step: 1100/7371, Loss: 0.8305\n","Epoch: 06, Step: 1200/7371, Loss: 0.8297\n","Epoch: 06, Step: 1300/7371, Loss: 0.8322\n","Epoch: 06, Step: 1400/7371, Loss: 0.8236\n","Epoch: 06, Step: 1500/7371, Loss: 0.8296\n","Epoch: 06, Step: 1600/7371, Loss: 0.8191\n","Epoch: 06, Step: 1700/7371, Loss: 0.8298\n","Epoch: 06, Step: 1800/7371, Loss: 0.8262\n","Epoch: 06, Step: 1900/7371, Loss: 0.8167\n","Epoch: 06, Step: 2000/7371, Loss: 0.8286\n","Epoch: 06, Step: 2100/7371, Loss: 0.8205\n","Epoch: 06, Step: 2200/7371, Loss: 0.8236\n","Epoch: 06, Step: 2300/7371, Loss: 0.8303\n","Epoch: 06, Step: 2400/7371, Loss: 0.8211\n","Epoch: 06, Step: 2500/7371, Loss: 0.8204\n","Epoch: 06, Step: 2600/7371, Loss: 0.8305\n","Epoch: 06, Step: 2700/7371, Loss: 0.8229\n","Epoch: 06, Step: 2800/7371, Loss: 0.8193\n","Epoch: 06, Step: 2900/7371, Loss: 0.8311\n","Epoch: 06, Step: 3000/7371, Loss: 0.8311\n","Epoch: 06, Step: 3100/7371, Loss: 0.8263\n","Epoch: 06, Step: 3200/7371, Loss: 0.8246\n","Epoch: 06, Step: 3300/7371, Loss: 0.8270\n","Epoch: 06, Step: 3400/7371, Loss: 0.8333\n","Epoch: 06, Step: 3500/7371, Loss: 0.8237\n","Epoch: 06, Step: 3600/7371, Loss: 0.8375\n","Epoch: 06, Step: 3700/7371, Loss: 0.8305\n","Epoch: 06, Step: 3800/7371, Loss: 0.8292\n","Epoch: 06, Step: 3900/7371, Loss: 0.8470\n","Epoch: 06, Step: 4000/7371, Loss: 0.8314\n","Epoch: 06, Step: 4100/7371, Loss: 0.8280\n","Epoch: 06, Step: 4200/7371, Loss: 0.8346\n","Epoch: 06, Step: 4300/7371, Loss: 0.8355\n","Epoch: 06, Step: 4400/7371, Loss: 0.8290\n","Epoch: 06, Step: 4500/7371, Loss: 0.8342\n","Epoch: 06, Step: 4600/7371, Loss: 0.8316\n","Epoch: 06, Step: 4700/7371, Loss: 0.8334\n","Epoch: 06, Step: 4800/7371, Loss: 0.8320\n","Epoch: 06, Step: 4900/7371, Loss: 0.8340\n","Epoch: 06, Step: 5000/7371, Loss: 0.8372\n","Epoch: 06, Step: 5100/7371, Loss: 0.8386\n","Epoch: 06, Step: 5200/7371, Loss: 0.8404\n","Epoch: 06, Step: 5300/7371, Loss: 0.8292\n","Epoch: 06, Step: 5400/7371, Loss: 0.8409\n","Epoch: 06, Step: 5500/7371, Loss: 0.8426\n","Epoch: 06, Step: 5600/7371, Loss: 0.8474\n","Epoch: 06, Step: 5700/7371, Loss: 0.8399\n","Epoch: 06, Step: 5800/7371, Loss: 0.8417\n","Epoch: 06, Step: 5900/7371, Loss: 0.8294\n","Epoch: 06, Step: 6000/7371, Loss: 0.8402\n","Epoch: 06, Step: 6100/7371, Loss: 0.8396\n","Epoch: 06, Step: 6200/7371, Loss: 0.8440\n","Epoch: 06, Step: 6300/7371, Loss: 0.8411\n","Epoch: 06, Step: 6400/7371, Loss: 0.8485\n","Epoch: 06, Step: 6500/7371, Loss: 0.8502\n","Epoch: 06, Step: 6600/7371, Loss: 0.8348\n","Epoch: 06, Step: 6700/7371, Loss: 0.8444\n","Epoch: 06, Step: 6800/7371, Loss: 0.8480\n","Epoch: 06, Step: 6900/7371, Loss: 0.8412\n","Epoch: 06, Step: 7000/7371, Loss: 0.8475\n","Epoch: 06, Step: 7100/7371, Loss: 0.8402\n","Epoch: 06, Step: 7200/7371, Loss: 0.8418\n","Epoch: 06, Step: 7300/7371, Loss: 0.8395\n"," 60%|████████████████████▍             | 6/10 [02:56<01:56, 29.23s/it]Epoch: 07, Step: 100/7371, Loss: 0.8321\n","Epoch: 07, Step: 200/7371, Loss: 0.8168\n","Epoch: 07, Step: 300/7371, Loss: 0.8201\n","Epoch: 07, Step: 400/7371, Loss: 0.8261\n","Epoch: 07, Step: 500/7371, Loss: 0.8298\n","Epoch: 07, Step: 600/7371, Loss: 0.8224\n","Epoch: 07, Step: 700/7371, Loss: 0.8271\n","Epoch: 07, Step: 800/7371, Loss: 0.8208\n","Epoch: 07, Step: 900/7371, Loss: 0.8167\n","Epoch: 07, Step: 1000/7371, Loss: 0.8256\n","Epoch: 07, Step: 1100/7371, Loss: 0.8215\n","Epoch: 07, Step: 1200/7371, Loss: 0.8222\n","Epoch: 07, Step: 1300/7371, Loss: 0.8219\n","Epoch: 07, Step: 1400/7371, Loss: 0.8216\n","Epoch: 07, Step: 1500/7371, Loss: 0.8245\n","Epoch: 07, Step: 1600/7371, Loss: 0.8253\n","Epoch: 07, Step: 1700/7371, Loss: 0.8301\n","Epoch: 07, Step: 1800/7371, Loss: 0.8227\n","Epoch: 07, Step: 1900/7371, Loss: 0.8285\n","Epoch: 07, Step: 2000/7371, Loss: 0.8259\n","Epoch: 07, Step: 2100/7371, Loss: 0.8255\n","Epoch: 07, Step: 2200/7371, Loss: 0.8337\n","Epoch: 07, Step: 2300/7371, Loss: 0.8299\n","Epoch: 07, Step: 2400/7371, Loss: 0.8266\n","Epoch: 07, Step: 2500/7371, Loss: 0.8245\n","Epoch: 07, Step: 2600/7371, Loss: 0.8357\n","Epoch: 07, Step: 2700/7371, Loss: 0.8301\n","Epoch: 07, Step: 2800/7371, Loss: 0.8331\n","Epoch: 07, Step: 2900/7371, Loss: 0.8268\n","Epoch: 07, Step: 3000/7371, Loss: 0.8328\n","Epoch: 07, Step: 3100/7371, Loss: 0.8275\n","Epoch: 07, Step: 3200/7371, Loss: 0.8294\n","Epoch: 07, Step: 3300/7371, Loss: 0.8279\n","Epoch: 07, Step: 3400/7371, Loss: 0.8268\n","Epoch: 07, Step: 3500/7371, Loss: 0.8250\n","Epoch: 07, Step: 3600/7371, Loss: 0.8341\n","Epoch: 07, Step: 3700/7371, Loss: 0.8364\n","Epoch: 07, Step: 3800/7371, Loss: 0.8319\n","Epoch: 07, Step: 3900/7371, Loss: 0.8246\n","Epoch: 07, Step: 4000/7371, Loss: 0.8291\n","Epoch: 07, Step: 4100/7371, Loss: 0.8313\n","Epoch: 07, Step: 4200/7371, Loss: 0.8364\n","Epoch: 07, Step: 4300/7371, Loss: 0.8407\n","Epoch: 07, Step: 4400/7371, Loss: 0.8342\n","Epoch: 07, Step: 4500/7371, Loss: 0.8282\n","Epoch: 07, Step: 4600/7371, Loss: 0.8329\n","Epoch: 07, Step: 4700/7371, Loss: 0.8263\n","Epoch: 07, Step: 4800/7371, Loss: 0.8257\n","Epoch: 07, Step: 4900/7371, Loss: 0.8392\n","Epoch: 07, Step: 5000/7371, Loss: 0.8321\n","Epoch: 07, Step: 5100/7371, Loss: 0.8329\n","Epoch: 07, Step: 5200/7371, Loss: 0.8323\n","Epoch: 07, Step: 5300/7371, Loss: 0.8419\n","Epoch: 07, Step: 5400/7371, Loss: 0.8318\n","Epoch: 07, Step: 5500/7371, Loss: 0.8343\n","Epoch: 07, Step: 5600/7371, Loss: 0.8391\n","Epoch: 07, Step: 5700/7371, Loss: 0.8416\n","Epoch: 07, Step: 5800/7371, Loss: 0.8385\n","Epoch: 07, Step: 5900/7371, Loss: 0.8444\n","Epoch: 07, Step: 6000/7371, Loss: 0.8330\n","Epoch: 07, Step: 6100/7371, Loss: 0.8450\n","Epoch: 07, Step: 6200/7371, Loss: 0.8357\n","Epoch: 07, Step: 6300/7371, Loss: 0.8346\n","Epoch: 07, Step: 6400/7371, Loss: 0.8477\n","Epoch: 07, Step: 6500/7371, Loss: 0.8355\n","Epoch: 07, Step: 6600/7371, Loss: 0.8415\n","Epoch: 07, Step: 6700/7371, Loss: 0.8388\n","Epoch: 07, Step: 6800/7371, Loss: 0.8488\n","Epoch: 07, Step: 6900/7371, Loss: 0.8441\n","Epoch: 07, Step: 7000/7371, Loss: 0.8510\n","Epoch: 07, Step: 7100/7371, Loss: 0.8434\n","Epoch: 07, Step: 7200/7371, Loss: 0.8392\n","Epoch: 07, Step: 7300/7371, Loss: 0.8453\n"," 70%|███████████████████████▊          | 7/10 [03:25<01:26, 28.98s/it]Epoch: 08, Step: 100/7371, Loss: 0.8230\n","Epoch: 08, Step: 200/7371, Loss: 0.8143\n","Epoch: 08, Step: 300/7371, Loss: 0.8200\n","Epoch: 08, Step: 400/7371, Loss: 0.8135\n","Epoch: 08, Step: 500/7371, Loss: 0.8232\n","Epoch: 08, Step: 600/7371, Loss: 0.8224\n","Epoch: 08, Step: 700/7371, Loss: 0.8235\n","Epoch: 08, Step: 800/7371, Loss: 0.8202\n","Epoch: 08, Step: 900/7371, Loss: 0.8173\n","Epoch: 08, Step: 1000/7371, Loss: 0.8173\n","Epoch: 08, Step: 1100/7371, Loss: 0.8346\n","Epoch: 08, Step: 1200/7371, Loss: 0.8290\n","Epoch: 08, Step: 1300/7371, Loss: 0.8154\n","Epoch: 08, Step: 1400/7371, Loss: 0.8355\n","Epoch: 08, Step: 1500/7371, Loss: 0.8176\n","Epoch: 08, Step: 1600/7371, Loss: 0.8198\n","Epoch: 08, Step: 1700/7371, Loss: 0.8352\n","Epoch: 08, Step: 1800/7371, Loss: 0.8296\n","Epoch: 08, Step: 1900/7371, Loss: 0.8306\n","Epoch: 08, Step: 2000/7371, Loss: 0.8257\n","Epoch: 08, Step: 2100/7371, Loss: 0.8281\n","Epoch: 08, Step: 2200/7371, Loss: 0.8286\n","Epoch: 08, Step: 2300/7371, Loss: 0.8282\n","Epoch: 08, Step: 2400/7371, Loss: 0.8221\n","Epoch: 08, Step: 2500/7371, Loss: 0.8299\n","Epoch: 08, Step: 2600/7371, Loss: 0.8282\n","Epoch: 08, Step: 2700/7371, Loss: 0.8295\n","Epoch: 08, Step: 2800/7371, Loss: 0.8283\n","Epoch: 08, Step: 2900/7371, Loss: 0.8257\n","Epoch: 08, Step: 3000/7371, Loss: 0.8288\n","Epoch: 08, Step: 3100/7371, Loss: 0.8258\n","Epoch: 08, Step: 3200/7371, Loss: 0.8274\n","Epoch: 08, Step: 3300/7371, Loss: 0.8284\n","Epoch: 08, Step: 3400/7371, Loss: 0.8276\n","Epoch: 08, Step: 3500/7371, Loss: 0.8244\n","Epoch: 08, Step: 3600/7371, Loss: 0.8310\n","Epoch: 08, Step: 3700/7371, Loss: 0.8301\n","Epoch: 08, Step: 3800/7371, Loss: 0.8369\n","Epoch: 08, Step: 3900/7371, Loss: 0.8312\n","Epoch: 08, Step: 4000/7371, Loss: 0.8302\n","Epoch: 08, Step: 4100/7371, Loss: 0.8309\n","Epoch: 08, Step: 4200/7371, Loss: 0.8309\n","Epoch: 08, Step: 4300/7371, Loss: 0.8320\n","Epoch: 08, Step: 4400/7371, Loss: 0.8359\n","Epoch: 08, Step: 4500/7371, Loss: 0.8288\n","Epoch: 08, Step: 4600/7371, Loss: 0.8309\n","Epoch: 08, Step: 4700/7371, Loss: 0.8372\n","Epoch: 08, Step: 4800/7371, Loss: 0.8348\n","Epoch: 08, Step: 4900/7371, Loss: 0.8340\n","Epoch: 08, Step: 5000/7371, Loss: 0.8305\n","Epoch: 08, Step: 5100/7371, Loss: 0.8310\n","Epoch: 08, Step: 5200/7371, Loss: 0.8438\n","Epoch: 08, Step: 5300/7371, Loss: 0.8297\n","Epoch: 08, Step: 5400/7371, Loss: 0.8387\n","Epoch: 08, Step: 5500/7371, Loss: 0.8313\n","Epoch: 08, Step: 5600/7371, Loss: 0.8350\n","Epoch: 08, Step: 5700/7371, Loss: 0.8435\n","Epoch: 08, Step: 5800/7371, Loss: 0.8447\n","Epoch: 08, Step: 5900/7371, Loss: 0.8468\n","Epoch: 08, Step: 6000/7371, Loss: 0.8389\n","Epoch: 08, Step: 6100/7371, Loss: 0.8310\n","Epoch: 08, Step: 6200/7371, Loss: 0.8392\n","Epoch: 08, Step: 6300/7371, Loss: 0.8350\n","Epoch: 08, Step: 6400/7371, Loss: 0.8442\n","Epoch: 08, Step: 6500/7371, Loss: 0.8443\n","Epoch: 08, Step: 6600/7371, Loss: 0.8371\n","Epoch: 08, Step: 6700/7371, Loss: 0.8491\n","Epoch: 08, Step: 6800/7371, Loss: 0.8449\n","Epoch: 08, Step: 6900/7371, Loss: 0.8454\n","Epoch: 08, Step: 7000/7371, Loss: 0.8446\n","Epoch: 08, Step: 7100/7371, Loss: 0.8489\n","Epoch: 08, Step: 7200/7371, Loss: 0.8440\n","Epoch: 08, Step: 7300/7371, Loss: 0.8456\n"," 80%|███████████████████████████▏      | 8/10 [03:54<00:58, 29.14s/it]Epoch: 09, Step: 100/7371, Loss: 0.8214\n","Epoch: 09, Step: 200/7371, Loss: 0.8272\n","Epoch: 09, Step: 300/7371, Loss: 0.8198\n","Epoch: 09, Step: 400/7371, Loss: 0.8105\n","Epoch: 09, Step: 500/7371, Loss: 0.8287\n","Epoch: 09, Step: 600/7371, Loss: 0.8265\n","Epoch: 09, Step: 700/7371, Loss: 0.8224\n","Epoch: 09, Step: 800/7371, Loss: 0.8261\n","Epoch: 09, Step: 900/7371, Loss: 0.8231\n","Epoch: 09, Step: 1000/7371, Loss: 0.8159\n","Epoch: 09, Step: 1100/7371, Loss: 0.8272\n","Epoch: 09, Step: 1200/7371, Loss: 0.8223\n","Epoch: 09, Step: 1300/7371, Loss: 0.8184\n","Epoch: 09, Step: 1400/7371, Loss: 0.8205\n","Epoch: 09, Step: 1500/7371, Loss: 0.8292\n","Epoch: 09, Step: 1600/7371, Loss: 0.8223\n","Epoch: 09, Step: 1700/7371, Loss: 0.8249\n","Epoch: 09, Step: 1800/7371, Loss: 0.8172\n","Epoch: 09, Step: 1900/7371, Loss: 0.8297\n","Epoch: 09, Step: 2000/7371, Loss: 0.8214\n","Epoch: 09, Step: 2100/7371, Loss: 0.8249\n","Epoch: 09, Step: 2200/7371, Loss: 0.8233\n","Epoch: 09, Step: 2300/7371, Loss: 0.8260\n","Epoch: 09, Step: 2400/7371, Loss: 0.8286\n","Epoch: 09, Step: 2500/7371, Loss: 0.8293\n","Epoch: 09, Step: 2600/7371, Loss: 0.8292\n","Epoch: 09, Step: 2700/7371, Loss: 0.8216\n","Epoch: 09, Step: 2800/7371, Loss: 0.8259\n","Epoch: 09, Step: 2900/7371, Loss: 0.8213\n","Epoch: 09, Step: 3000/7371, Loss: 0.8288\n","Epoch: 09, Step: 3100/7371, Loss: 0.8235\n","Epoch: 09, Step: 3200/7371, Loss: 0.8261\n","Epoch: 09, Step: 3300/7371, Loss: 0.8329\n","Epoch: 09, Step: 3400/7371, Loss: 0.8214\n","Epoch: 09, Step: 3500/7371, Loss: 0.8306\n","Epoch: 09, Step: 3600/7371, Loss: 0.8304\n","Epoch: 09, Step: 3700/7371, Loss: 0.8344\n","Epoch: 09, Step: 3800/7371, Loss: 0.8306\n","Epoch: 09, Step: 3900/7371, Loss: 0.8202\n","Epoch: 09, Step: 4000/7371, Loss: 0.8317\n","Epoch: 09, Step: 4100/7371, Loss: 0.8246\n","Epoch: 09, Step: 4200/7371, Loss: 0.8329\n","Epoch: 09, Step: 4300/7371, Loss: 0.8368\n","Epoch: 09, Step: 4400/7371, Loss: 0.8322\n","Epoch: 09, Step: 4500/7371, Loss: 0.8349\n","Epoch: 09, Step: 4600/7371, Loss: 0.8364\n","Epoch: 09, Step: 4700/7371, Loss: 0.8300\n","Epoch: 09, Step: 4800/7371, Loss: 0.8305\n","Epoch: 09, Step: 4900/7371, Loss: 0.8283\n","Epoch: 09, Step: 5000/7371, Loss: 0.8304\n","Epoch: 09, Step: 5100/7371, Loss: 0.8320\n","Epoch: 09, Step: 5200/7371, Loss: 0.8239\n","Epoch: 09, Step: 5300/7371, Loss: 0.8399\n","Epoch: 09, Step: 5400/7371, Loss: 0.8318\n","Epoch: 09, Step: 5500/7371, Loss: 0.8317\n","Epoch: 09, Step: 5600/7371, Loss: 0.8360\n","Epoch: 09, Step: 5700/7371, Loss: 0.8399\n","Epoch: 09, Step: 5800/7371, Loss: 0.8357\n","Epoch: 09, Step: 5900/7371, Loss: 0.8399\n","Epoch: 09, Step: 6000/7371, Loss: 0.8375\n","Epoch: 09, Step: 6100/7371, Loss: 0.8404\n","Epoch: 09, Step: 6200/7371, Loss: 0.8356\n","Epoch: 09, Step: 6300/7371, Loss: 0.8389\n","Epoch: 09, Step: 6400/7371, Loss: 0.8350\n","Epoch: 09, Step: 6500/7371, Loss: 0.8348\n","Epoch: 09, Step: 6600/7371, Loss: 0.8436\n","Epoch: 09, Step: 6700/7371, Loss: 0.8450\n","Epoch: 09, Step: 6800/7371, Loss: 0.8447\n","Epoch: 09, Step: 6900/7371, Loss: 0.8458\n","Epoch: 09, Step: 7000/7371, Loss: 0.8390\n","Epoch: 09, Step: 7100/7371, Loss: 0.8343\n","Epoch: 09, Step: 7200/7371, Loss: 0.8445\n","Epoch: 09, Step: 7300/7371, Loss: 0.8436\n"," 90%|██████████████████████████████▌   | 9/10 [04:22<00:28, 28.77s/it]Epoch: 10, Step: 100/7371, Loss: 0.8203\n","Epoch: 10, Step: 200/7371, Loss: 0.8289\n","Epoch: 10, Step: 300/7371, Loss: 0.8220\n","Epoch: 10, Step: 400/7371, Loss: 0.8268\n","Epoch: 10, Step: 500/7371, Loss: 0.8243\n","Epoch: 10, Step: 600/7371, Loss: 0.8196\n","Epoch: 10, Step: 700/7371, Loss: 0.8274\n","Epoch: 10, Step: 800/7371, Loss: 0.8300\n","Epoch: 10, Step: 900/7371, Loss: 0.8240\n","Epoch: 10, Step: 1000/7371, Loss: 0.8198\n","Epoch: 10, Step: 1100/7371, Loss: 0.8223\n","Epoch: 10, Step: 1200/7371, Loss: 0.8115\n","Epoch: 10, Step: 1300/7371, Loss: 0.8238\n","Epoch: 10, Step: 1400/7371, Loss: 0.8200\n","Epoch: 10, Step: 1500/7371, Loss: 0.8286\n","Epoch: 10, Step: 1600/7371, Loss: 0.8149\n","Epoch: 10, Step: 1700/7371, Loss: 0.8215\n","Epoch: 10, Step: 1800/7371, Loss: 0.8180\n","Epoch: 10, Step: 1900/7371, Loss: 0.8289\n","Epoch: 10, Step: 2000/7371, Loss: 0.8258\n","Epoch: 10, Step: 2100/7371, Loss: 0.8236\n","Epoch: 10, Step: 2200/7371, Loss: 0.8221\n","Epoch: 10, Step: 2300/7371, Loss: 0.8234\n","Epoch: 10, Step: 2400/7371, Loss: 0.8147\n","Epoch: 10, Step: 2500/7371, Loss: 0.8231\n","Epoch: 10, Step: 2600/7371, Loss: 0.8196\n","Epoch: 10, Step: 2700/7371, Loss: 0.8228\n","Epoch: 10, Step: 2800/7371, Loss: 0.8285\n","Epoch: 10, Step: 2900/7371, Loss: 0.8260\n","Epoch: 10, Step: 3000/7371, Loss: 0.8259\n","Epoch: 10, Step: 3100/7371, Loss: 0.8183\n","Epoch: 10, Step: 3200/7371, Loss: 0.8292\n","Epoch: 10, Step: 3300/7371, Loss: 0.8337\n","Epoch: 10, Step: 3400/7371, Loss: 0.8312\n","Epoch: 10, Step: 3500/7371, Loss: 0.8215\n","Epoch: 10, Step: 3600/7371, Loss: 0.8225\n","Epoch: 10, Step: 3700/7371, Loss: 0.8280\n","Epoch: 10, Step: 3800/7371, Loss: 0.8205\n","Epoch: 10, Step: 3900/7371, Loss: 0.8330\n","Epoch: 10, Step: 4000/7371, Loss: 0.8303\n","Epoch: 10, Step: 4100/7371, Loss: 0.8295\n","Epoch: 10, Step: 4200/7371, Loss: 0.8434\n","Epoch: 10, Step: 4300/7371, Loss: 0.8258\n","Epoch: 10, Step: 4400/7371, Loss: 0.8364\n","Epoch: 10, Step: 4500/7371, Loss: 0.8357\n","Epoch: 10, Step: 4600/7371, Loss: 0.8351\n","Epoch: 10, Step: 4700/7371, Loss: 0.8257\n","Epoch: 10, Step: 4800/7371, Loss: 0.8349\n","Epoch: 10, Step: 4900/7371, Loss: 0.8266\n","Epoch: 10, Step: 5000/7371, Loss: 0.8342\n","Epoch: 10, Step: 5100/7371, Loss: 0.8393\n","Epoch: 10, Step: 5200/7371, Loss: 0.8290\n","Epoch: 10, Step: 5300/7371, Loss: 0.8304\n","Epoch: 10, Step: 5400/7371, Loss: 0.8351\n","Epoch: 10, Step: 5500/7371, Loss: 0.8369\n","Epoch: 10, Step: 5600/7371, Loss: 0.8365\n","Epoch: 10, Step: 5700/7371, Loss: 0.8345\n","Epoch: 10, Step: 5800/7371, Loss: 0.8344\n","Epoch: 10, Step: 5900/7371, Loss: 0.8290\n","Epoch: 10, Step: 6000/7371, Loss: 0.8404\n","Epoch: 10, Step: 6100/7371, Loss: 0.8367\n","Epoch: 10, Step: 6200/7371, Loss: 0.8347\n","Epoch: 10, Step: 6300/7371, Loss: 0.8373\n","Epoch: 10, Step: 6400/7371, Loss: 0.8395\n","Epoch: 10, Step: 6500/7371, Loss: 0.8435\n","Epoch: 10, Step: 6600/7371, Loss: 0.8425\n","Epoch: 10, Step: 6700/7371, Loss: 0.8408\n","Epoch: 10, Step: 6800/7371, Loss: 0.8408\n","Epoch: 10, Step: 6900/7371, Loss: 0.8465\n","Epoch: 10, Step: 7000/7371, Loss: 0.8341\n","Epoch: 10, Step: 7100/7371, Loss: 0.8393\n","Epoch: 10, Step: 7200/7371, Loss: 0.8499\n","Epoch: 10, Step: 7300/7371, Loss: 0.8563\n","100%|█████████████████████████████████| 10/10 [04:55<00:00, 29.52s/it]\n","Finish prepping n2v embeddings\n","Setting up Train data\n","Setting up Val data\n","Setting up Test data\n","Total Prep time: 303.4804175859995 sec\n","S3GRLLight selected\n","Model architecture is: S3GRLLight(\n","  (operator_diff): MLP(34, 1024)\n","  (link_pred_mlp): MLP(2048, 1024, 1)\n",")\n","Total number of parameters is 5913025\n"," 41%|███████▊           | 15787/38405 [1:53:04<1737:35:14, 276.56s/it]"]}]}]}